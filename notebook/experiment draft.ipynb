{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78885e4d",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "\n",
    "# Ignore warnings for cleaner outputs\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================================\n",
    "# Heading for Data Visualization with Plotly\n",
    "# ==================================\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b4779",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7801a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hao Le\\AppData\\Local\\Temp\\ipykernel_29988\\2155449455.py:1: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_daily_rent_detail= pd.read_csv(\"../data/raw/daily_rent_detail.csv\")\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.08 GiB for an array with shape (9, 16086672) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_daily_rent_detail= \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/raw/daily_rent_detail.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df_station_list= pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/raw/station_list.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df_usage_frequency= pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/raw/usage_frequency.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    776\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    777\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    778\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2163\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2161\u001b[39m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[32m0\u001b[39m].shape, axes, e)\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[32m-> \u001b[39m\u001b[32m2163\u001b[39m     \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1807\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1802\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1809\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2288\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2286\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2287\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2288\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2289\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2291\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Hao Le\\MASTER\\Data Mining\\Final project\\bike-sharing-demand-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2320\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2317\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2319\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2321\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2323\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.08 GiB for an array with shape (9, 16086672) and data type object"
     ]
    }
   ],
   "source": [
    "df_daily_rent_detail= pd.read_csv(\"../data/raw/daily_rent_detail.csv\")\n",
    "df_station_list= pd.read_csv(\"../data/raw/station_list.csv\")\n",
    "df_usage_frequency= pd.read_csv(\"../data/raw/usage_frequency.csv\")\n",
    "df_weather= pd.read_csv(\"../data/raw/weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3701adaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946D42AD89539210</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-05-30 17:25:29</td>\n",
       "      <td>2020-05-31 18:25:22</td>\n",
       "      <td>Anacostia Library</td>\n",
       "      <td>31804</td>\n",
       "      <td>11th &amp; H St NE</td>\n",
       "      <td>31614.0</td>\n",
       "      <td>38.865784</td>\n",
       "      <td>-76.978400</td>\n",
       "      <td>38.899983</td>\n",
       "      <td>-76.991383</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC46FAAB662B8613</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-05-09 14:42:04</td>\n",
       "      <td>2020-05-09 15:06:33</td>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>31256</td>\n",
       "      <td>21st St &amp; Constitution Ave NW</td>\n",
       "      <td>31261.0</td>\n",
       "      <td>38.895914</td>\n",
       "      <td>-77.026064</td>\n",
       "      <td>38.892459</td>\n",
       "      <td>-77.046567</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72F00B2FB833D6ED</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-05-24 17:27:19</td>\n",
       "      <td>2020-05-24 17:43:51</td>\n",
       "      <td>Connecticut Ave &amp; Newark St NW / Cleveland Park</td>\n",
       "      <td>31305</td>\n",
       "      <td>12th &amp; U St NW</td>\n",
       "      <td>31268.0</td>\n",
       "      <td>38.934267</td>\n",
       "      <td>-77.057979</td>\n",
       "      <td>38.916787</td>\n",
       "      <td>-77.028139</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4DFBE6AED989DF35</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-05-27 15:29:52</td>\n",
       "      <td>2020-05-27 15:47:13</td>\n",
       "      <td>Connecticut Ave &amp; Newark St NW / Cleveland Park</td>\n",
       "      <td>31305</td>\n",
       "      <td>14th &amp; Belmont St NW</td>\n",
       "      <td>31119.0</td>\n",
       "      <td>38.934267</td>\n",
       "      <td>-77.057979</td>\n",
       "      <td>38.921074</td>\n",
       "      <td>-77.031887</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1AAFE6B4331AB9DF</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-05-31 14:06:03</td>\n",
       "      <td>2020-05-31 14:30:30</td>\n",
       "      <td>Georgia Ave &amp; Morton St NW</td>\n",
       "      <td>31419</td>\n",
       "      <td>17th &amp; K St NW</td>\n",
       "      <td>31213.0</td>\n",
       "      <td>38.932128</td>\n",
       "      <td>-77.023500</td>\n",
       "      <td>38.902760</td>\n",
       "      <td>-77.038630</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id rideable_type           started_at             ended_at  \\\n",
       "0  946D42AD89539210   docked_bike  2020-05-30 17:25:29  2020-05-31 18:25:22   \n",
       "1  CC46FAAB662B8613   docked_bike  2020-05-09 14:42:04  2020-05-09 15:06:33   \n",
       "2  72F00B2FB833D6ED   docked_bike  2020-05-24 17:27:19  2020-05-24 17:43:51   \n",
       "3  4DFBE6AED989DF35   docked_bike  2020-05-27 15:29:52  2020-05-27 15:47:13   \n",
       "4  1AAFE6B4331AB9DF   docked_bike  2020-05-31 14:06:03  2020-05-31 14:30:30   \n",
       "\n",
       "                                start_station_name start_station_id  \\\n",
       "0                                Anacostia Library            31804   \n",
       "1                                   10th & E St NW            31256   \n",
       "2  Connecticut Ave & Newark St NW / Cleveland Park            31305   \n",
       "3  Connecticut Ave & Newark St NW / Cleveland Park            31305   \n",
       "4                       Georgia Ave & Morton St NW            31419   \n",
       "\n",
       "                end_station_name end_station_id  start_lat  start_lng  \\\n",
       "0                 11th & H St NE        31614.0  38.865784 -76.978400   \n",
       "1  21st St & Constitution Ave NW        31261.0  38.895914 -77.026064   \n",
       "2                 12th & U St NW        31268.0  38.934267 -77.057979   \n",
       "3           14th & Belmont St NW        31119.0  38.934267 -77.057979   \n",
       "4                 17th & K St NW        31213.0  38.932128 -77.023500   \n",
       "\n",
       "     end_lat    end_lng member_casual  \n",
       "0  38.899983 -76.991383        casual  \n",
       "1  38.892459 -77.046567        member  \n",
       "2  38.916787 -77.028139        member  \n",
       "3  38.921074 -77.031887        casual  \n",
       "4  38.902760 -77.038630        casual  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily_rent_detail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e6f0b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16086672 entries, 0 to 16086671\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 1.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_daily_rent_detail.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea62b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.608666e+07</td>\n",
       "      <td>1.608666e+07</td>\n",
       "      <td>1.606074e+07</td>\n",
       "      <td>1.606074e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.890389e+01</td>\n",
       "      <td>-7.703200e+01</td>\n",
       "      <td>3.890290e+01</td>\n",
       "      <td>-7.703162e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.758673e-02</td>\n",
       "      <td>3.436006e-02</td>\n",
       "      <td>4.704474e-02</td>\n",
       "      <td>8.250517e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.873000e+01</td>\n",
       "      <td>-7.740000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.810000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.889054e+01</td>\n",
       "      <td>-7.704463e+01</td>\n",
       "      <td>3.889050e+01</td>\n",
       "      <td>-7.704468e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.890304e+01</td>\n",
       "      <td>-7.703150e+01</td>\n",
       "      <td>3.890257e+01</td>\n",
       "      <td>-7.703000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.891554e+01</td>\n",
       "      <td>-7.701350e+01</td>\n",
       "      <td>3.891305e+01</td>\n",
       "      <td>-7.701237e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.914000e+01</td>\n",
       "      <td>-7.678414e+01</td>\n",
       "      <td>4.307000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          start_lat     start_lng       end_lat       end_lng\n",
       "count  1.608666e+07  1.608666e+07  1.606074e+07  1.606074e+07\n",
       "mean   3.890389e+01 -7.703200e+01  3.890290e+01 -7.703162e+01\n",
       "std    2.758673e-02  3.436006e-02  4.704474e-02  8.250517e-02\n",
       "min    3.873000e+01 -7.740000e+01  0.000000e+00 -7.810000e+01\n",
       "25%    3.889054e+01 -7.704463e+01  3.889050e+01 -7.704468e+01\n",
       "50%    3.890304e+01 -7.703150e+01  3.890257e+01 -7.703000e+01\n",
       "75%    3.891554e+01 -7.701350e+01  3.891305e+01 -7.701237e+01\n",
       "max    3.914000e+01 -7.678414e+01  4.307000e+01  0.000000e+00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily_rent_detail.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42907f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                         Info of Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "                         Sample of 8 Rows\n",
      "======================================================================\n",
      "                   ride_id  rideable_type           started_at  \\\n",
      "9318650   3F8AAFE3BE86D8B3   classic_bike  2023-05-01 22:23:15   \n",
      "5698269   0027FE0346F1CE86    docked_bike  2022-06-27 12:33:41   \n",
      "10045511  A4C8AFA681B72BEA  electric_bike  2023-07-01 14:29:59   \n",
      "2559771   7B0EC3C49BD1914C   classic_bike  2021-06-27 08:37:00   \n",
      "504230    4E6E150799B877F0    docked_bike  2020-07-19 17:19:55   \n",
      "9647261   BF1E8E5759095FC3  electric_bike  2023-06-25 01:34:45   \n",
      "7650156   77B5F6022D0A26FD   classic_bike  2022-11-08 18:31:11   \n",
      "10596022  CD6666585F86ADCB   classic_bike  2023-08-30 09:51:06   \n",
      "\n",
      "                     ended_at                           start_station_name  \\\n",
      "9318650   2023-05-01 22:26:47                          14th & Newton St NW   \n",
      "5698269   2022-06-27 12:50:01  Henry Bacon Dr & Lincoln Memorial Circle NW   \n",
      "10045511  2023-07-01 15:12:48                       19th St N & Ft Myer Dr   \n",
      "2559771   2021-06-27 09:05:49                                2nd & G St NE   \n",
      "504230    2020-07-19 17:38:12                    Jefferson Dr & 14th St SW   \n",
      "9647261   2023-06-25 02:02:53                                          NaN   \n",
      "7650156   2022-11-08 18:40:24                                4th & G St SW   \n",
      "10596022  2023-08-30 09:56:59               17th St & Massachusetts Ave NW   \n",
      "\n",
      "         start_station_id                end_station_name end_station_id  \\\n",
      "9318650           31649.0             16th & Irving St NW        31122.0   \n",
      "5698269           31289.0                  22nd & H St NW        31127.0   \n",
      "10045511          31014.0     Arlington National Cemetery        31978.0   \n",
      "2559771           31639.0                Lincoln Memorial        31258.0   \n",
      "504230            31247.0  15th St & Massachusetts Ave SE        31626.0   \n",
      "9647261               NaN                             NaN            NaN   \n",
      "7650156           31666.0        New Jersey Ave & H St SE        31828.0   \n",
      "10596022          31267.0   18th St & Pennsylvania Ave NW        31242.0   \n",
      "\n",
      "          start_lat  start_lng    end_lat    end_lng member_casual  \n",
      "9318650   38.931991 -77.032956  38.928893 -77.036250        member  \n",
      "5698269   38.890539 -77.049383  38.898925 -77.048852        casual  \n",
      "10045511  38.897326 -77.072153  38.884533 -77.065803        casual  \n",
      "2559771   38.899670 -77.003666  38.888255 -77.049436        member  \n",
      "504230    38.888553 -77.032429  38.887320 -76.983569        member  \n",
      "9647261   38.880000 -77.010000  38.870000 -77.010000        member  \n",
      "7650156   38.881117 -77.017699  38.880761 -77.005741        member  \n",
      "10596022  38.908142 -77.038359  38.899680 -77.041539        member  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         DataFrame Info\n",
      "======================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16086672 entries, 0 to 16086671\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 1.6+ GB\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         Percentage of Missing Values\n",
      "======================================================================\n",
      "ride_id               0.000000\n",
      "rideable_type         0.000000\n",
      "started_at            0.000000\n",
      "ended_at              0.000000\n",
      "start_station_name    9.014605\n",
      "start_station_id      9.014605\n",
      "end_station_name      9.686702\n",
      "end_station_id        9.690737\n",
      "start_lat             0.000062\n",
      "start_lng             0.000062\n",
      "end_lat               0.161183\n",
      "end_lng               0.161183\n",
      "member_casual         0.000000\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         Shape of DataFrame\n",
      "======================================================================\n",
      "(16086672, 13)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         Columns of DataFrame\n",
      "======================================================================\n",
      "Index(['ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
      "       'start_station_name', 'start_station_id', 'end_station_name',\n",
      "       'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
      "       'member_casual'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         Count of Duplicated Rows\n",
      "======================================================================\n",
      "0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         Descriptive Statistics\n",
      "======================================================================\n",
      "          start_lat     start_lng       end_lat       end_lng\n",
      "count  1.608666e+07  1.608666e+07  1.606074e+07  1.606074e+07\n",
      "mean   3.890389e+01 -7.703200e+01  3.890290e+01 -7.703162e+01\n",
      "std    2.758673e-02  3.436006e-02  4.704474e-02  8.250517e-02\n",
      "min    3.873000e+01 -7.740000e+01  0.000000e+00 -7.810000e+01\n",
      "25%    3.889054e+01 -7.704463e+01  3.889050e+01 -7.704468e+01\n",
      "50%    3.890304e+01 -7.703150e+01  3.890257e+01 -7.703000e+01\n",
      "75%    3.891554e+01 -7.701350e+01  3.891305e+01 -7.701237e+01\n",
      "max    3.914000e+01 -7.678414e+01  4.307000e+01  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def print_heading(title):\n",
    "    \"\"\"Prints a formatted heading.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\" \" * 25 + title)\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def print_dataset_info(df):\n",
    "    \"\"\"Prints various information about the dataset.\"\"\"\n",
    "    # Main heading\n",
    "    print_heading(\"Info of Dataset\")\n",
    "\n",
    "    # Sample 8 rows from the DataFrame\n",
    "    print_heading(\"Sample of 8 Rows\")\n",
    "    print(df.sample(8))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display DataFrame info\n",
    "    print_heading(\"DataFrame Info\")\n",
    "    df.info()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Percentage of missing values\n",
    "    print_heading(\"Percentage of Missing Values\")\n",
    "    missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "    print(missing_percentage)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Shape of the DataFrame\n",
    "    print_heading(\"Shape of DataFrame\")\n",
    "    print(df.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Columns of the DataFrame\n",
    "    print_heading(\"Columns of DataFrame\")\n",
    "    print(df.columns)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check for duplicated rows\n",
    "    print_heading(\"Count of Duplicated Rows\")\n",
    "    print(df.duplicated().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Describe the DataFrame\n",
    "    print_heading(\"Descriptive Statistics\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    \n",
    "    \n",
    "print_dataset_info(df_daily_rent_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb92836",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beeed804",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Cấu hình giao diện biểu đồ cho đẹp\u001b[39;00m\n\u001b[32m      5\u001b[39m sns.set_theme(style=\u001b[33m\"\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mplt\u001b[49m.rcParams[\u001b[33m'\u001b[39m\u001b[33mfigure.figsize\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Giả sử dữ liệu của bạn đang nằm trong biến df\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Nếu chưa load, bỏ comment dòng dưới:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# df = pd.read_csv('hour.csv') \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ::20 nghĩa là: Lấy từ đầu đến cuối, cứ cách 20 dòng lấy 1 dòng\u001b[39;00m\n\u001b[32m     16\u001b[39m df_raw = df_daily_rent_detail.iloc[::\u001b[32m20\u001b[39m].copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "\n",
    "# Cấu hình giao diện biểu đồ cho đẹp\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Giả sử dữ liệu của bạn đang nằm trong biến df\n",
    "# Nếu chưa load, bỏ comment dòng dưới:\n",
    "# df = pd.read_csv('hour.csv') \n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 0: LOAD DỮ LIỆU (ĐÃ SỬA ĐỂ KHÔNG BỊ TRÀN RAM)\n",
    "# ==============================================================================\n",
    "# ::20 nghĩa là: Lấy từ đầu đến cuối, cứ cách 20 dòng lấy 1 dòng\n",
    "df_raw = df_daily_rent_detail.iloc[::20].copy()\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(f\"Kích thước df_daily_rent_detail gốc: {df_daily_rent_detail.shape}\")\n",
    "print(f\"Kích thước df_raw (sau lọc): {df_raw.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 1: KHÁM PHÁ CẤU TRÚC DỮ LIỆU (STRUCTURE EXPLORATION)\n",
    "# ==============================================================================\n",
    "print(\"=\"*30 + \" BƯỚC 1: KHÁM PHÁ CẤU TRÚC \" + \"=\"*30)\n",
    "\n",
    "# 1.1. Số bản ghi và số biến\n",
    "print(f\"Kích thước dữ liệu (Dòng, Cột): {df_raw.shape}\")\n",
    "\n",
    "# 1.2. Kiểm tra kiểu dữ liệu\n",
    "print(\"\\n--- Thông tin kiểu dữ liệu ---\")\n",
    "print(df_raw.info())\n",
    "\n",
    "# 1.3. Xử lý dữ liệu thời gian (Nếu cột ngày tháng chưa phải là datetime)\n",
    "# (Thay 'dteday' bằng tên cột ngày tháng thực tế của bạn)\n",
    "if 'started_at' in df_raw.columns:\n",
    "    df_raw['started_at'] = pd.to_datetime(df_raw['started_at'], format='mixed')\n",
    "    # Bổ sung cột năm, tháng, thứ nếu thiếu\n",
    "    df_raw['year'] = df_raw['started_at'].dt.year\n",
    "    df_raw['month'] = df_raw['started_at'].dt.month\n",
    "    df_raw['day_of_week'] = df_raw['started_at'].dt.day_name()\n",
    "# 1.4. Thống kê phân bố theo thời gian\n",
    "print(\"\\n--- Phân bố dữ liệu theo Năm ---\")\n",
    "print(df_raw['year'].value_counts().sort_index())\n",
    "print(\"\\n--- Phân bố dữ liệu theo Tháng ---\")\n",
    "print(df_raw['month'].value_counts().sort_index())\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 2: THỐNG KÊ MÔ TẢ & TRỰC QUAN HÓA (DESCRIPTIVE & VISUALIZATION)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*30 + \" BƯỚC 2: THỐNG KÊ MÔ TẢ & TRỰC QUAN \" + \"=\"*30)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.1. BIỂU ĐỒ PHÂN BỐ CỦA BIẾN MỤC TIÊU (ĐÃ SỬA LỖI CHO RAW DATA)\n",
    "# ==============================================================================\n",
    "# Biến mục tiêu là 'cnt' (Số lượng xe thuê trong một khoảng thời gian, ví dụ 1 giờ)\n",
    "print(\"\\n--- Biểu đồ phân bố của biến mục tiêu 'cnt' ---\")\n",
    "\n",
    "# 1. Đảm bảo cột thời gian đã đúng định dạng\n",
    "col_time = 'started_at'  # Tên cột trong file Raw của bạn\n",
    "# Chuyển đổi sang datetime, xử lý lỗi format mixed\n",
    "df_raw[col_time] = pd.to_datetime(df_raw[col_time], format='mixed', errors='coerce')\n",
    "\n",
    "# 2. TÍNH TOÁN 'cnt': Gom nhóm theo từng Giờ cụ thể (Date + Hour)\n",
    "# Tạo cột tạm 'date_hour' để làm tròn thời gian về đầu giờ\n",
    "df_raw['date_hour'] = df_raw[col_time].dt.floor('h')\n",
    "\n",
    "# Đếm số dòng trong mỗi giờ -> Đây chính là 'cnt' (Lượng cầu)\n",
    "df_hourly_counts = df_raw.groupby('date_hour').size().reset_index(name='cnt')\n",
    "\n",
    "# 3. Vẽ biểu đồ Histogram phân phối\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_hourly_counts['cnt'], kde=True, bins=30, color='blue')\n",
    "plt.title('Phân phối lượng thuê xe mỗi giờ (Tính từ dữ liệu Raw)', fontsize=14)\n",
    "plt.xlabel('Số lượng xe thuê (cnt)')\n",
    "plt.ylabel('Tần suất')\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.2. PHÂN TÍCH THEO THỜI GIAN (TIME SERIES ANALYSIS)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Vẽ biểu đồ Time Series đa chiều ---\")\n",
    "\n",
    "# Chuẩn bị dữ liệu cho các trục vẽ (Feature Extraction)\n",
    "df_raw['hour'] = df_raw[col_time].dt.hour\n",
    "df_raw['month'] = df_raw[col_time].dt.month\n",
    "df_raw['year'] = df_raw[col_time].dt.year\n",
    "df_raw['day_of_week'] = df_raw[col_time].dt.day_name()\n",
    "\n",
    "# Tạo cột 'workingday' (Ngày làm vs Cuối tuần) để vẽ biểu đồ 1\n",
    "# 0-4 là Thứ 2-Thứ 6 (Đi làm), 5-6 là T7-CN (Nghỉ)\n",
    "df_raw['is_workingday'] = df_raw[col_time].dt.dayofweek.apply(lambda x: 'Ngày làm việc' if x <= 4 else 'Cuối tuần/Lễ')\n",
    "\n",
    "# --- Khởi tạo khung 4 biểu đồ (2 dòng, 2 cột) ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# BIỂU ĐỒ 1: Theo Giờ (So sánh Ngày làm vs Cuối tuần)\n",
    "# -----------------------------------------------------------\n",
    "# Cần gom nhóm trước khi vẽ lineplot để nhẹ máy và đúng ý nghĩa\n",
    "plot_data_hour = df_raw.groupby(['hour', 'is_workingday']).size().reset_index(name='cnt')\n",
    "\n",
    "sns.lineplot(ax=axes[0,0], data=plot_data_hour, x='hour', y='cnt', hue='is_workingday', marker='o', palette=['blue', 'red'])\n",
    "axes[0,0].set_title('Nhu cầu thuê xe trung bình theo Giờ', fontsize=12)\n",
    "axes[0,0].set_xticks(range(0, 24))\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# BIỂU ĐỒ 2: Theo Tháng\n",
    "# -----------------------------------------------------------\n",
    "plot_data_month = df_raw.groupby('month').size().reset_index(name='cnt')\n",
    "\n",
    "sns.barplot(ax=axes[0,1], data=plot_data_month, x='month', y='cnt', palette='viridis')\n",
    "axes[0,1].set_title('Tổng lượng thuê xe theo Tháng', fontsize=12)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# BIỂU ĐỒ 3: Theo Thứ trong tuần (Boxplot)\n",
    "# -----------------------------------------------------------\n",
    "# Để vẽ Boxplot, ta cần dữ liệu tổng hợp theo NGÀY (Daily Counts)\n",
    "df_raw['date_only'] = df_raw[col_time].dt.date\n",
    "# Đếm tổng xe mỗi ngày\n",
    "daily_counts = df_raw.groupby(['date_only', 'day_of_week']).size().reset_index(name='cnt')\n",
    "\n",
    "order_day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sns.boxplot(ax=axes[1,0], data=daily_counts, x='day_of_week', y='cnt', order=order_day, palette='Set2')\n",
    "axes[1,0].set_title('Phân bố lượng thuê xe theo Thứ', fontsize=12)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# BIỂU ĐỒ 4: Theo Năm (Trend tăng trưởng)\n",
    "# -----------------------------------------------------------\n",
    "plot_data_year = df_raw.groupby('year').size().reset_index(name='cnt')\n",
    "\n",
    "sns.barplot(ax=axes[1,1], data=plot_data_year, x='year', y='cnt', palette='magma')\n",
    "axes[1,1].set_title('Tổng lượng thuê xe theo Năm', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 3: KIỂM TRA CHẤT LƯỢNG DỮ LIỆU (DATA QUALITY)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*30 + \" BƯỚC 3: KIỂM TRA CHẤT LƯỢNG DỮ LIỆU \" + \"=\"*30)\n",
    "\n",
    "# 1. Đảm bảo cột thời gian đúng định dạng (để kiểm tra logic thời gian)\n",
    "# Lưu ý: Sửa tên cột 'started_at', 'ended_at' cho đúng với file của bạn\n",
    "if 'started_at' in df_raw.columns and 'ended_at' in df_raw.columns:\n",
    "    df_raw['started_at'] = pd.to_datetime(df_raw['started_at'], format='mixed', errors='coerce')\n",
    "    df_raw['ended_at'] = pd.to_datetime(df_raw['ended_at'], format='mixed', errors='coerce')\n",
    "\n",
    "# --- 3.1. Kiểm tra Missing Values (Giá trị thiếu) ---\n",
    "print(\"--- Số lượng giá trị thiếu (Missing Values) ---\")\n",
    "missing = df_raw.isnull().sum()\n",
    "# Chỉ in ra những cột có dữ liệu bị thiếu\n",
    "print(missing[missing > 0]) \n",
    "\n",
    "# --- 3.2. Kiểm tra dữ liệu trùng lặp (Duplicate Rides) ---\n",
    "# Với Raw Data, mỗi chuyến đi có 1 'ride_id' duy nhất. Cần kiểm tra trùng ID.\n",
    "if 'ride_id' in df_raw.columns:\n",
    "    duplicates = df_raw.duplicated(subset=['ride_id']).sum()\n",
    "    print(f\"\\n--- Số chuyến đi bị trùng lặp ID: {duplicates} ---\")\n",
    "else:\n",
    "    duplicates = df_raw.duplicated().sum()\n",
    "    print(f\"\\n--- Số dòng bị trùng lặp hoàn toàn: {duplicates} ---\")\n",
    "\n",
    "# --- 3.3. Kiểm tra Logic Thời gian (Thay thế cho check 'cnt=0') ---\n",
    "# Lỗi thường gặp: Thời gian kết thúc sớm hơn thời gian bắt đầu (Duration bị âm)\n",
    "if 'started_at' in df_raw.columns and 'ended_at' in df_raw.columns:\n",
    "    # Tính thời lượng chuyến đi (phút)\n",
    "    df_raw['duration_min'] = (df_raw['ended_at'] - df_raw['started_at']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Tìm các chuyến đi bị lỗi (Thời lượng <= 0 hoặc quá dài > 24h)\n",
    "    invalid_rides = df_raw[(df_raw['duration_min'] <= 0) | (df_raw['duration_min'] > 1440)]\n",
    "    \n",
    "    print(f\"\\n--- Số chuyến đi lỗi thời gian (Âm hoặc > 24h): {len(invalid_rides)} ---\")\n",
    "    if len(invalid_rides) > 0:\n",
    "        print(invalid_rides[['started_at', 'ended_at', 'duration_min']].head())\n",
    "\n",
    "# --- 3.4. Kiểm tra Logic Tọa độ (Thay thế cho check 'temp/weather') ---\n",
    "# Dữ liệu raw chưa có thời tiết, nhưng có tọa độ (Lat/Lng). Kiểm tra tọa độ = 0 hoặc null.\n",
    "cols_geo = ['start_lat', 'start_lng', 'end_lat', 'end_lng']\n",
    "existing_geo = [c for c in cols_geo if c in df_raw.columns]\n",
    "\n",
    "if existing_geo:\n",
    "    print(\"\\n--- Thống kê nhanh Tọa độ (Check Min/Max để tìm điểm bất thường) ---\")\n",
    "    print(df_raw[existing_geo].describe())\n",
    "    \n",
    "    # Ví dụ: Latitude của Mỹ thường là 38.x, nếu thấy số 0 hoặc số lạ là lỗi\n",
    "    invalid_geo = df_raw[(df_raw['start_lat'] == 0) | (df_raw['start_lng'] == 0)]\n",
    "    print(f\"Số dòng có tọa độ bằng 0: {len(invalid_geo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb451b7",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da45746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a41aad9f",
   "metadata": {},
   "source": [
    "# Phân cụm theo giờ + khu vực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã lưu bản đồ thành công vào file: ban_do_ket_qua.html\n",
      "👉 Hãy mở thư mục chứa code của bạn, tìm file 'ban_do_ket_qua.html' và click đúp để mở nó bằng Chrome/Edge.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. CHUẨN BỊ DỮ LIỆU\n",
    "# ---------------------------------------------------------\n",
    "df_daily_rent_detail = pd.read_csv(\n",
    "    \"../data/raw/daily_rent_detail.csv\", \n",
    "    dtype={5: str, 7: str} # Ép cột 5 và 7 thành chuỗi (text)\n",
    ")\n",
    "df_usage_frequency= pd.read_csv(\"../data/raw/usage_frequency.csv\")\n",
    "\n",
    "# Lấy tọa độ\n",
    "starts = df_daily_rent_detail[['start_station_name', 'start_lat', 'start_lng']].rename(\n",
    "    columns={'start_station_name': 'station_name', 'start_lat': 'lat', 'start_lng': 'lng'})\n",
    "ends = df_daily_rent_detail[['end_station_name', 'end_lat', 'end_lng']].rename(\n",
    "    columns={'end_station_name': 'station_name', 'end_lat': 'lat', 'end_lng': 'lng'})\n",
    "station_coords = pd.concat([starts, ends]).drop_duplicates(subset=['station_name']).dropna()\n",
    "\n",
    "# Lấy lượng xe (Drop-off)\n",
    "station_stats = df_usage_frequency.groupby('station_name')['dropoff_counts'].sum().reset_index()\n",
    "\n",
    "# Gộp dữ liệu\n",
    "df_final = pd.merge(station_stats, station_coords, on='station_name', how='inner')\n",
    "\n",
    "# 2. MÁY TÍNH TOÁN (PHÂN CỤM & GẮN NHÃN TỰ ĐỘNG)\n",
    "# ---------------------------------------------------------\n",
    "# Phân thành 3 cụm dựa trên vị trí\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df_final['cluster'] = kmeans.fit_predict(df_final[['lat', 'lng']])\n",
    "\n",
    "# Tính trung bình lượng xe mỗi cụm để xác định \"nhãn\"\n",
    "cluster_avg = df_final.groupby('cluster')['dropoff_counts'].mean().sort_values()\n",
    "# Logic: Ít xe -> Ngoại ô, Nhiều xe -> Thành thị\n",
    "cluster_names = {\n",
    "    cluster_avg.index[0]: 'Ngoại vi (Ít xe)',\n",
    "    cluster_avg.index[1]: 'Cận trung tâm (Vừa)',\n",
    "    cluster_avg.index[2]: 'Trung tâm (Sầm uất)'\n",
    "}\n",
    "df_final['label'] = df_final['cluster'].map(cluster_names)\n",
    "\n",
    "# Màu sắc cho từng cụm để vẽ bản đồ\n",
    "colors = {\n",
    "    'Ngoại vi (Ít xe)': 'green',      # Màu xanh lá cây\n",
    "    'Cận trung tâm (Vừa)': 'orange',  # Màu cam\n",
    "    'Trung tâm (Sầm uất)': 'red'      # Màu đỏ\n",
    "}\n",
    "\n",
    "# 3. VẼ BẢN ĐỒ FOLIUM (ĐỂ CON NGƯỜI KIỂM TRA)\n",
    "# ---------------------------------------------------------\n",
    "# Tạo bản đồ nền Washington DC\n",
    "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12)\n",
    "\n",
    "# Vẽ từng trạm xe lên bản đồ\n",
    "for idx, row in df_final.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        radius=row['dropoff_counts'] / 2000, # Bán kính hình tròn to nhỏ theo lượng xe\n",
    "        popup=f\"Trạm: {row['station_name']}<br>Khu vực: {row['label']}<br>Lượng xe: {row['dropoff_counts']}\",\n",
    "        color=colors[row['label']],\n",
    "        fill=True,\n",
    "        fill_color=colors[row['label']],\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "# Lệnh lưu bản đồ ra file riêng\n",
    "output_file = \"ban_do_ket_qua.html\"\n",
    "m.save(output_file)\n",
    "\n",
    "print(f\"✅ Đã lưu bản đồ thành công vào file: {output_file}\")\n",
    "print(\"👉 Hãy mở thư mục chứa code của bạn, tìm file 'ban_do_ket_qua.html' và click đúp để mở nó bằng Chrome/Edge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a880341c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_weather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1.1 Chuẩn hóa ngày tháng để merge\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_weather[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf_weather\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m df_usage_frequency[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df_usage_frequency[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1.2 Xử lý tọa độ trạm (Lấy danh sách trạm duy nhất)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_weather' is not defined"
     ]
    }
   ],
   "source": [
    "# 1.1 Chuẩn hóa ngày tháng để merge\n",
    "df_weather['datetime'] = pd.to_datetime(df_weather['datetime'])\n",
    "df_usage_frequency['date'] = pd.to_datetime(df_usage_frequency['date'])\n",
    "\n",
    "# 1.2 Xử lý tọa độ trạm (Lấy danh sách trạm duy nhất)\n",
    "starts = df_daily_rent_detail[['start_station_name', 'start_lat', 'start_lng']].rename(\n",
    "    columns={'start_station_name': 'station_name', 'start_lat': 'lat', 'start_lng': 'lng'})\n",
    "ends = df_daily_rent_detail[['end_station_name', 'end_lat', 'end_lng']].rename(\n",
    "    columns={'end_station_name': 'station_name', 'end_lat': 'lat', 'end_lng': 'lng'})\n",
    "station_coords = pd.concat([starts, ends]).drop_duplicates(subset=['station_name']).dropna()\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 2: FEATURE ENGINEERING & SỬA SAI DỮ LIỆU (QUAN TRỌNG)\n",
    "# ==============================================================================\n",
    "print(\"--- 2. ĐANG SỬA SAI DỮ LIỆU (DE-BIASING) ---\")\n",
    "\n",
    "# 2.1 Tính tổng nhu cầu lịch sử để phân cụm\n",
    "total_stats = df_usage_frequency.groupby('station_name')['pickup_counts'].sum().reset_index()\n",
    "df_merged = pd.merge(total_stats, station_coords, on='station_name', how='inner')\n",
    "\n",
    "# 2.2 Phân cụm địa lý (Tìm hàng xóm)\n",
    "# Nếu ít dữ liệu quá thì giảm n_clusters\n",
    "n_clusters = 5 if len(df_merged) > 10 else 2 \n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df_merged['cluster'] = kmeans.fit_predict(df_merged[['lat', 'lng']])\n",
    "\n",
    "# 2.3 Tính hệ số điều chỉnh (Correction Factor)\n",
    "# Logic: Nếu trạm này kém hơn Top 20% hàng xóm -> Cần nhân hệ số lên\n",
    "benchmark = df_merged.groupby('cluster')['pickup_counts'].transform(lambda x: x.quantile(0.8))\n",
    "df_merged['correction_factor'] = np.where(\n",
    "    df_merged['pickup_counts'] < benchmark, \n",
    "    benchmark / df_merged['pickup_counts'], # Tỷ lệ điều chỉnh\n",
    "    1.0 # Nếu đã tốt thì giữ nguyên\n",
    ")\n",
    "# Giới hạn hệ số trần (cap) để tránh nhân quá lớn (ví dụ max x3)\n",
    "df_merged['correction_factor'] = df_merged['correction_factor'].clip(upper=3.0)\n",
    "\n",
    "# 2.4 Áp dụng hệ số vào dữ liệu từng ngày (Daily Data)\n",
    "# Merge lại vào bảng usage chi tiết\n",
    "full_data = df_usage_frequency.merge(df_merged[['station_name', 'cluster', 'correction_factor']], on='station_name', how='inner')\n",
    "\n",
    "# Tạo cột mục tiêu mới: Adjusted Demand (Nhu cầu đã sửa)\n",
    "full_data['adjusted_demand'] = full_data['pickup_counts'] * full_data['correction_factor']\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 3: MERGE THỜI TIẾT & EDA NHANH\n",
    "# ==============================================================================\n",
    "print(\"--- 3. ĐANG KẾT HỢP BIẾN THỜI TIẾT ---\")\n",
    "\n",
    "# Merge với weather\n",
    "dataset = pd.merge(full_data, df_weather, left_on='date', right_on='datetime', how='inner')\n",
    "\n",
    "# Chọn các biến quan trọng để train\n",
    "features = ['temp', 'humidity', 'precip', 'windspeed', 'cluster'] # X\n",
    "target = 'adjusted_demand' # y (Dùng cái này thay vì pickup_counts gốc)\n",
    "\n",
    "# EDA: Vẽ ma trận tương quan (Correlation Matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Chỉ lấy các cột số để tính correlation\n",
    "corr_cols = features + ['pickup_counts', 'adjusted_demand']\n",
    "sns.heatmap(dataset[corr_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Tương quan giữa Thời tiết và Nhu cầu (Gốc vs Điều chỉnh)')\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 4: TRAIN MÔ HÌNH DỰ BÁO (MACHINE LEARNING)\n",
    "# ==============================================================================\n",
    "print(\"--- 4. HUẤN LUYỆN MÔ HÌNH ---\")\n",
    "\n",
    "# Xử lý dữ liệu thiếu (nếu có)\n",
    "dataset = dataset.dropna(subset=features)\n",
    "\n",
    "X = dataset[features]\n",
    "y = dataset[target]\n",
    "\n",
    "# Chia tập Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Khởi tạo và train mô hình Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ==============================================================================\n",
    "# BƯỚC 5: ĐÁNH GIÁ KẾT QUẢ\n",
    "# ==============================================================================\n",
    "print(\"--- 5. KẾT QUẢ DỰ BÁO ---\")\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Độ chính xác (R2 Score): {r2:.4f}\")\n",
    "print(f\"Sai số trung bình (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Vẽ biểu đồ so sánh Thực tế vs Dự báo (trên tập Test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3, color='blue')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Đường chuẩn\n",
    "plt.xlabel('Nhu cầu thực tế (Đã điều chỉnh)')\n",
    "plt.ylabel('Nhu cầu dự báo (Model)')\n",
    "plt.title('Đánh giá mô hình: Dự báo vs Thực tế')\n",
    "plt.show()\n",
    "\n",
    "# Xem biến nào quan trọng nhất (Feature Importance)\n",
    "importances = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "print(\"\\nCác yếu tố ảnh hưởng nhất đến lượng xe thuê:\")\n",
    "print(importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
